{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization: *Part 1*\n",
    "#### **Extreme Outlier Removal**: Analyzing relative extremes across Primary Activites, z-Score analysis, IQR (Identifying outlier \"bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import f_oneway\n",
    "from scipy.stats import zscore\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "data = pd.read_csv('merged_data.csv')\n",
    "\n",
    "# create dataframe\n",
    "data['Scope_3_emissions_amount'] = pd.to_numeric(data['Scope_3_emissions_amount'], errors='coerce')\n",
    "# remove nulls\n",
    "data = data.dropna(subset=['Scope_3_emissions_amount', 'account_id'])\n",
    "# remove scientific notation\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)  \n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of rows less than or equal to 0\n",
    "zeros = (data['Scope_3_emissions_amount']<= 0).sum()\n",
    "print(\"Total Zeros:  \" + str(zeros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe with rows where column value <= 0\n",
    "new_df = data[data['Scope_3_emissions_amount'] <= 0]\n",
    "\n",
    "# Check how many rows meet this condition:\n",
    "print(\"Number of rows where column_name <= 0:\", len(new_df))\n",
    "print(\"Percentage of total rows:\", (len(new_df)/len(data))*100, \"%\")\n",
    "print(\"\\nValue counts:\")\n",
    "print(new_df['Scope_3_emissions_amount'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe with rows where column value <= 0\n",
    "data_clean = data[data['Scope_3_emissions_amount'] > 0]\n",
    "\n",
    "# To see how many rows meet this condition:\n",
    "print(\"Number of rows where column_name <= 0:\", len(data_clean))\n",
    "print(\"Percentage of total rows:\", (len(data_clean)/len(data))*100, \"%\")\n",
    "\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print column names\n",
    "data_clean.columns.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with select columns\n",
    "outlier_df = data_clean[['account_name'\n",
    "                         , 'account_id'\n",
    "                         , 'Year'\n",
    "                         , 'incorporated_country'\n",
    "                         , 'Primary activity'\n",
    "                         , 'Primary sector'\n",
    "                         , 'Market_Cap_USD'\n",
    "                         , 'Third_party_verification'\n",
    "                         , 'Revenue_USD'\n",
    "                         , 'ebitda_USD'\n",
    "                         , 'grossProfit_USD'\n",
    "                         , 'netIncome_USD'\n",
    "                         , 'cashAndCashEquivalents_USD'\n",
    "                         , 'shortTermInvestments_USD'\n",
    "                         , 'longTermInvestments_USD'\n",
    "                         , 'totalAssets_USD'\n",
    "                         , 'totalLiabilities_USD'\n",
    "                         , 'totalInvestments_USD'\n",
    "                         , 'totalDebt_USD'\n",
    "                         , 'totalEquity_USD'\n",
    "                         , 'country_gdp'\n",
    "                         , 'country_total_ghg'\n",
    "                         , 'country_population'\n",
    "                         , 'Scope_3_emissions_type'\n",
    "                         , 'Scope_3_emissions_amount']] \n",
    "outlier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null scope 3 emissions observations\n",
    "outlier_df['Scope_3_emissions_amount'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Capture Scope 3 Amount Skewness & Kurtosis Before Outlier Removal*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate skewness and kurtosis\n",
    "original_skew = outlier_df['Scope_3_emissions_amount'].skew()\n",
    "original_kurtosis = outlier_df['Scope_3_emissions_amount'].kurtosis()\n",
    "print(\"Skew: \") \n",
    "print(original_skew)\n",
    "print(\"Kurtosis: \") \n",
    "print(original_kurtosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Observation Volume*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GHG data includes data entries from 2013 to 2023, however, not all companies have reported every single year. In order to have sufficinet data, I am going to narrow the years down to a range with the most volume and exclude accounts that do not report for all of those range years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the amount of records for each year\n",
    "outlier_df.groupby('Year').size().plot(kind='bar')\n",
    "plt.title('Unique Rows by Year')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected, the number of entreies grows each year. Given this, I will keep the accounts with records in every year from 2018 to 2023. This will allow for 5 years of training data to predict the final year (2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get account_ids that appear in all years\n",
    "valid_accounts = outlier_df.groupby('account_id')['Year'].unique().apply(set)\n",
    "valid_accounts = valid_accounts[valid_accounts.apply(lambda x: all(year in x for year in range(2018, 2024)))].index\n",
    "valid_accounts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe to keep only those account_ids\n",
    "outlier_df = outlier_df[outlier_df['account_id'].isin(valid_accounts)]\n",
    "outlier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df['Year'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify final result\n",
    "outlier_df = outlier_df[(outlier_df['Year'] >= 2018)]\n",
    "print(outlier_df['Year'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how large is the dataset\n",
    "outlier_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA & Normalization : Extreme Variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been narrowed down to a smaller scope of, I want to look for extreme variation in account reporting year over year within a primary activity. Through the process of EDA in Tableau, it was discovered that some accounts report extremely high and extremely low numbers for the same primary activity across different years. Because it is impossible to validate any level of accuracy in this instance, I believe these accounts should be removed entirely from the data. This function should assist in determining these cases for further analysis and removal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_extreme_variations(outlier_df, value_column, year_column='Year', \n",
    "                          activity_column='Primary activity', \n",
    "                          account_column='account_id',\n",
    "                          z_score_threshold=2):\n",
    "    \n",
    "    # Calculate variation metrics for each account within a primary activity\n",
    "    variation_stats = (outlier_df.groupby([activity_column, account_column])\n",
    "                      .agg({\n",
    "                          value_column: ['std', 'mean', 'min', 'max', 'count'],\n",
    "                          year_column: list\n",
    "                      })\n",
    "                      .reset_index())\n",
    "    \n",
    "    # Flatten column names\n",
    "    variation_stats.columns = [\n",
    "        f\"{col[0]}_{col[1]}\" if col[1] else col[0] \n",
    "        for col in variation_stats.columns\n",
    "    ]\n",
    "    \n",
    "    # Calculate coefficient of variation (CV)\n",
    "    variation_stats['cv'] = (variation_stats[f'{value_column}_std'] / \n",
    "                           variation_stats[f'{value_column}_mean'].abs())\n",
    "    \n",
    "    # Calculate range ratio\n",
    "    variation_stats['range_ratio'] = (variation_stats[f'{value_column}_max'].abs() / \n",
    "                                    variation_stats[f'{value_column}_min'].abs())\n",
    "    \n",
    "    # Calculate Z-scores of CV within each Primary Activity\n",
    "    variation_stats['cv_zscore'] = (variation_stats\n",
    "                                   .groupby(activity_column)['cv']\n",
    "                                   .transform(lambda x: stats.zscore(x)))\n",
    "    \n",
    "    # Identify extreme accounts\n",
    "    extreme_accounts = variation_stats[\n",
    "        (variation_stats['cv_zscore'].abs() > z_score_threshold) &\n",
    "        (variation_stats[f'{value_column}_count'] > 1)  # At least 2 years of data\n",
    "    ].copy()\n",
    "    \n",
    "    # Sort and format results\n",
    "    extreme_accounts = extreme_accounts.sort_values(\n",
    "        ['cv_zscore'], \n",
    "        ascending=False\n",
    "    )\n",
    "    \n",
    "    # Add year range information\n",
    "    extreme_accounts['year_range'] = extreme_accounts[f'{year_column}_list'].apply(\n",
    "        lambda x: f\"{min(x)}-{max(x)}\"\n",
    "    )\n",
    "    \n",
    "    return extreme_accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function\n",
    "results = find_extreme_variations(\n",
    "    outlier_df,\n",
    "    value_column='Scope_3_emissions_amount',\n",
    "    z_score_threshold=2\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For every account and primary activity combo where extreme variation was found, plot the YoY trend:\n",
    "\n",
    "def create_plots(df, results):\n",
    "    # Filter dataframe to only include accounts that are in results\n",
    "    filtered_df = outlier_df[outlier_df['account_id'].isin(results['account_id'])]\n",
    "    \n",
    "    # Get unique combinations of account_id and Primary activity\n",
    "    unique_combinations = filtered_df[['account_id', 'Primary activity']].drop_duplicates()\n",
    "    \n",
    "    # Create a separate plot for each combination\n",
    "    for _, row in unique_combinations.iterrows():\n",
    "        account = row['account_id']\n",
    "        activity = row['Primary activity']\n",
    "        \n",
    "        # Filter data for this specific combination\n",
    "        plot_data = filtered_df[\n",
    "            (filtered_df['account_id'] == account) & \n",
    "            (filtered_df['Primary activity'] == activity)\n",
    "        ]\n",
    "        \n",
    "        # Create individual plot\n",
    "        plt.figure(figsize=(4, 2))\n",
    "        sns.barplot(\n",
    "            data=plot_data,\n",
    "            x='Year',\n",
    "            y='Scope_3_emissions_amount',\n",
    "            errorbar=None\n",
    "        )\n",
    "        \n",
    "        # Customize plot\n",
    "        plt.title(f'Account: {account}\\nPrimary Activity: {activity}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Run function\n",
    "create_plots(outlier_df, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at each of these trends, it's clear there is very inconsistent reporting YoY and these should be excluded\n",
    "# Remove all 64 accounts that were found in the extreme variation function\n",
    "outlier_df = outlier_df[~outlier_df['account_id'].isin(results['account_id'])]\n",
    "outlier_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA and Normalization: Z-Score Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that accounts with unreliable/extreme variation have been removed, I will create a function to detect outliers within each Primary activity that falls above a z-score threshold of 3 to see what outliers remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store outlier records\n",
    "outlier_records = []\n",
    "\n",
    "# Process each Primary Activity\n",
    "for activity in outlier_df['Primary activity'].unique():\n",
    "    # Get data for this activity\n",
    "    activity_data = outlier_df[outlier_df['Primary activity'] == activity]\n",
    "    \n",
    "    # Calculate z-scores and std dev for this activity\n",
    "    z_scores = stats.zscore(activity_data['Scope_3_emissions_amount'])\n",
    "    std_dev = activity_data['Scope_3_emissions_amount'].std()\n",
    "    \n",
    "    # Find outliers (z-score >= 3)\n",
    "    outlier_mask = abs(z_scores) >= 3\n",
    "    \n",
    "    # Get outlier records\n",
    "    outliers = activity_data[outlier_mask].copy()\n",
    "    outliers['z_score'] = z_scores[outlier_mask]\n",
    "    outliers['std_dev'] = std_dev\n",
    "    \n",
    "    # Add to records list\n",
    "    outlier_records.append(outliers[['account_id', 'account_name', 'Primary activity', \n",
    "                                   'Scope_3_emissions_amount', 'Year', 'z_score', 'std_dev']])\n",
    "\n",
    "# Combine all outliers into one dataframe\n",
    "outliers = pd.concat(outlier_records)\n",
    "\n",
    "# Sort by absolute z-score (highest to lowest)\n",
    "outliers = outliers.sort_values(by='z_score', key=abs, ascending=False)\n",
    "\n",
    "# Reset index\n",
    "outliers = outliers.reset_index(drop=True)\n",
    "\n",
    "print(\"\\nOutliers Detail:\")\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualize the outliers YoY\n",
    "# Create the scatter plot\n",
    "fig = px.scatter(outliers, \n",
    "                 x='Year', \n",
    "                 y='z_score',\n",
    "                 color='Primary activity',\n",
    "                 hover_data={\n",
    "                     'account_name': True,\n",
    "                     'Scope_3_emissions_amount': ':.2f',\n",
    "                     'z_score': ':.2f',\n",
    "                     'Year': True\n",
    "                 },\n",
    "                 title='Scope 3 Emissions Outliers Over Time')\n",
    "\n",
    "# Add horizontal lines for z-score thresholds\n",
    "fig.add_hline(y=3, line_dash=\"dash\", line_color=\"red\", annotation_text=\"z-score = 3\")\n",
    "fig.add_hline(y=-3, line_dash=\"dash\", line_color=\"red\")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Year\",\n",
    "    yaxis_title=\"Z-Score\",\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most of the remaining 985 outliers fall under a z-score of 15, I will first remove accounts with a z-score of 15 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Primary activity and re-calculate z-scores within each group\n",
    "z_scores = outlier_df.groupby('Primary activity')['Scope_3_emissions_amount'].transform(\n",
    "    lambda x: abs((x - x.mean()) / x.std())\n",
    ")\n",
    "\n",
    "# Find accounts that have any z-score of 15 or higher\n",
    "outlier_accounts = outlier_df[z_scores >= 15]['account_id'].unique()\n",
    "\n",
    "# Remove all rows for these accounts\n",
    "outlier_df_cleaned = outlier_df[~outlier_df['account_id'].isin(outlier_accounts)].copy()\n",
    "\n",
    "# Print summary\n",
    "total_removed = len(outlier_df) - len(outlier_df_cleaned)\n",
    "accounts_removed = len(outlier_accounts)\n",
    "print(f\"Total accounts removed: {accounts_removed}\")\n",
    "print(f\"Total rows removed: {total_removed}\")\n",
    "\n",
    "# Show removal count by Primary activity\n",
    "removal_by_activity = (\n",
    "    outlier_df['Primary activity'].value_counts() - \n",
    "    outlier_df_cleaned['Primary activity'].value_counts()\n",
    ").fillna(0)\n",
    "print(\"\\nRows removed by Primary activity:\")\n",
    "print(removal_by_activity[removal_by_activity > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This removed 8 more accounts and a total of 320 rows. I will vizualize the final results looking at a scatterplot of every value and which percentile they land in to better understand the distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the percentiles\n",
    "percentiles = [25, 50, 80, 90, 95, 96, 97, 98, 99, 100]\n",
    "percentile_values = {p: outlier_df_cleaned['Scope_3_emissions_amount'].quantile(p/100) for p in percentiles}\n",
    "\n",
    "# Create color scale for different percentile ranges\n",
    "color_scale = ['#d4e6f1','#a9cce3','#7fb3d5','#5499c7','#2980b9', '#2471a3', '#1f618d', '#1a5276', '#1b4f72', '#641e16']\n",
    "\n",
    "# Initialize the figure\n",
    "fig = go.Figure()\n",
    "#1b4f72\n",
    "# Create percentile ranges and add traces for each range\n",
    "prev_percentile = 0\n",
    "for i, percentile in enumerate(percentiles):\n",
    "    if i == 0:\n",
    "        mask = outlier_df_cleaned['Scope_3_emissions_amount'] <= percentile_values[percentile]\n",
    "        start = 0\n",
    "    else:\n",
    "        mask = (outlier_df_cleaned['Scope_3_emissions_amount'] > percentile_values[prev_percentile]) & \\\n",
    "               (outlier_df_cleaned['Scope_3_emissions_amount'] <= percentile_values[percentile])\n",
    "        start = prev_percentile\n",
    "    \n",
    "    data = outlier_df_cleaned[mask].copy()\n",
    "    data['percentile_range'] = f\"{start}-{percentile}\"\n",
    "    \n",
    "    if len(data) > 0:  # Only add trace if there is data in this range\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=data.index,\n",
    "                y=data['Scope_3_emissions_amount'],\n",
    "                mode='markers',\n",
    "                name=f\"{start}-{percentile}th percentile\",\n",
    "                marker=dict(\n",
    "                    color=color_scale[i] if i < len(color_scale) else color_scale[-1],\n",
    "                    size=6\n",
    "                ),\n",
    "                hovertemplate=(\n",
    "                    '<b>Account ID:</b> %{customdata[0]}<br>' +\n",
    "                    '<b>Account Name:</b> %{customdata[1]}<br>' +\n",
    "                    '<b>Primary Activity:</b> %{customdata[2]}<br>' +\n",
    "                    '<b>Scope 3 Emissions:</b> %{y:,.2f}<br>' +\n",
    "                    '<b>Percentile Range:</b> %{customdata[3]}<br>'\n",
    "                ),\n",
    "                customdata=np.column_stack((\n",
    "                    data['account_id'],\n",
    "                    data['account_name'],\n",
    "                    data['Primary activity'],\n",
    "                    data['percentile_range']\n",
    "                ))\n",
    "            )\n",
    "        )\n",
    "    prev_percentile = percentile\n",
    "\n",
    "# Add lines for each percentile\n",
    "for percentile, value in percentile_values.items():\n",
    "    fig.add_hline(\n",
    "        y=value,\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"gray\",\n",
    "        line_width=1,\n",
    "        annotation=dict(\n",
    "            text=f\"{percentile}th percentile\",\n",
    "            xref=\"paper\",\n",
    "            yref=\"y\",\n",
    "            x=1.02,\n",
    "            y=value,\n",
    "            showarrow=False,\n",
    "            font=dict(size=8)\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    width=1200,\n",
    "    title_text=\"Scope 3 Emissions Distribution by Percentile Ranges\",\n",
    "    xaxis_title=\"Index\",\n",
    "    yaxis_title=\"Scope 3 Emissions Amount\",\n",
    "    showlegend=True,\n",
    "    legend_title=\"Percentile Ranges\",\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=1.02,\n",
    "        bgcolor=\"rgba(255, 255, 255, 0.8)\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "for p in percentiles:\n",
    "    print(f\"{p}th percentile: {percentile_values[p]:,.2f}\")\n",
    "print(f\"\\nTotal number of observations: {len(outlier_df_cleaned)}\")\n",
    "print(f\"Number of unique accounts: {outlier_df_cleaned['account_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many observations fall into each of the percentiles?\n",
    "\n",
    "# Initialize dictionary to store counts\n",
    "counts = {}\n",
    "\n",
    "# Calculate for first bin (0 to 25th percentile)\n",
    "mask = outlier_df_cleaned['Scope_3_emissions_amount'] <= percentile_values[25]\n",
    "counts['0-25'] = len(outlier_df_cleaned[mask])\n",
    "\n",
    "# Calculate for intermediate bins\n",
    "for i in range(len(percentiles)-1):\n",
    "    current_percentile = percentiles[i]\n",
    "    next_percentile = percentiles[i+1]\n",
    "    \n",
    "    mask = (outlier_df_cleaned['Scope_3_emissions_amount'] > percentile_values[current_percentile]) & \\\n",
    "           (outlier_df_cleaned['Scope_3_emissions_amount'] <= percentile_values[next_percentile])\n",
    "    \n",
    "    counts[f'{current_percentile}-{next_percentile}'] = len(outlier_df_cleaned[mask])\n",
    "\n",
    "# Print results with percentages\n",
    "total_observations = len(outlier_df_cleaned)\n",
    "print(\"\\nObservations in each percentile range:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Percentile Range':<20} {'Count':>10} {'Percentage':>12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for range_name, count in counts.items():\n",
    "    percentage = (count / total_observations) * 100\n",
    "    print(f\"{range_name:<20} {count:>10,} {percentage:>11.2f}%\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Total':<20} {total_observations:>10,} {100:>11.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There remains significant jump in the 95th percentile. I will vizualize the z-scores within each primary activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get unique primary activities\n",
    "activities = outlier_df_cleaned['Primary activity'].unique()\n",
    "\n",
    "# Calculate number of rows and columns for subplots\n",
    "n_plots = len(activities)\n",
    "n_cols = 3\n",
    "n_rows = (n_plots + n_cols - 1) // n_cols\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# For loop that will create a plot for each primary activity\n",
    "for idx, activity in enumerate(activities):\n",
    "    # Get data for activity\n",
    "    activity_data = outlier_df[outlier_df['Primary activity'] == activity]\n",
    "    \n",
    "    # Calculate z-scores for this activity\n",
    "    z_scores = stats.zscore(activity_data['Scope_3_emissions_amount'])\n",
    "    \n",
    "    # Create scatter plot\n",
    "    axs[idx].scatter(activity_data['Year'], z_scores, alpha=0.5)\n",
    "    \n",
    "    # Add threshold line\n",
    "    axs[idx].axhline(y=3, color='r', linestyle='--', label='Z-score = 3')\n",
    "    axs[idx].axhline(y=-3, color='r', linestyle='--')\n",
    "    \n",
    "    # Customize plot\n",
    "    axs[idx].set_title(f'Primary Activity: {activity}')\n",
    "    axs[idx].set_xlabel('Year')\n",
    "    axs[idx].set_ylabel('Z-score')\n",
    "    axs[idx].grid(True)\n",
    "    axs[idx].legend()\n",
    "\n",
    "# Remove empty subplots if any\n",
    "for idx in range(len(activities), len(axs)):\n",
    "    fig.delaxes(axs[idx])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still some significant outliers within almost every primary activity, so it is not a limited set of activities causing skew. I will do a quick binning to see how many outliers fall in different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "# Loop through each Primary Activity\n",
    "for activity in outlier_df['Primary activity'].unique():\n",
    "    # Get data for this activity\n",
    "    activity_data = outlier_df[outlier_df['Primary activity'] == activity]\n",
    "    \n",
    "    # Calculate z-scores\n",
    "    z_scores = stats.zscore(activity_data['Scope_3_emissions_amount'])\n",
    "    \n",
    "    # Count outliers in different ranges\n",
    "    low_outliers = sum((z_scores >= 3) & (z_scores < 5))\n",
    "    high_outliers = sum((z_scores >= 5) & (z_scores < 8))\n",
    "    extreme_outliers = sum(z_scores >= 8)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Primary Activity': activity,\n",
    "        'Outliers (3 to 5)': low_outliers,\n",
    "        'Outliers (5 to 8)': high_outliers,\n",
    "        'Outliers (8 or higher)': extreme_outliers,\n",
    "        'Total Outliers': low_outliers + high_outliers + extreme_outliers\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Calculate totals row\n",
    "totals = {\n",
    "    'Primary Activity': 'TOTAL',\n",
    "    'Outliers (3 to 5)': results_df['Outliers (3 to 5)'].sum(),\n",
    "    'Outliers (5 to 8)': results_df['Outliers (5 to 8)'].sum(),\n",
    "    'Outliers (8 or higher)': results_df['Outliers (8 or higher)'].sum(),\n",
    "    'Total Outliers': results_df['Total Outliers'].sum()\n",
    "}\n",
    "\n",
    "# Append totals row\n",
    "results_df = pd.concat([results_df, pd.DataFrame([totals])], ignore_index=True)\n",
    "\n",
    "print(\"\\nOutlier Counts by Primary Activity:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram and stats to look at the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the histogram\n",
    "sns.histplot(data=outlier_df_cleaned, x='Scope_3_emissions_amount', bins=40, kde=True)\n",
    "# Customize the plot\n",
    "plt.title('Distribution of Amount')\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Frequency')\n",
    "# Add grid for better readability\n",
    "plt.grid(True, alpha=0.3)\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# final df stats\n",
    "print(outlier_df_cleaned['Scope_3_emissions_amount'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many rows are above the 98th percentile and what is the value of that percentile?\n",
    "column_name = 'Above_98'\n",
    "percentile_98 = outlier_df_cleaned['Scope_3_emissions_amount'].quantile(0.98)\n",
    "count_above_98 = outlier_df_cleaned[outlier_df_cleaned['Scope_3_emissions_amount'] > percentile_98].shape[0]\n",
    "count_rows = outlier_df_cleaned['Scope_3_emissions_amount'].notna().sum()\n",
    "percent = count_above_98/count_rows*100\n",
    "\n",
    "print(f\"98th percentile value: {percentile_98}\")\n",
    "print(\"Total Rows in Data: \" + str(count_rows))\n",
    "print(f\"Number of rows above 98th percentile: {count_above_98}\")\n",
    "print(f\"% above 98th percentile: {percent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many rows are above the 99th percentile and what is the value of that percentile?\n",
    "column_name = 'Above_99'\n",
    "percentile_99 = outlier_df_cleaned['Scope_3_emissions_amount'].quantile(0.99)\n",
    "count_above_99 = outlier_df_cleaned[outlier_df_cleaned['Scope_3_emissions_amount'] > percentile_99].shape[0]\n",
    "count_rows = outlier_df_cleaned['Scope_3_emissions_amount'].notna().sum()\n",
    "percent = count_above_99/count_rows*100\n",
    "\n",
    "print(f\"99th percentile value: {percentile_99}\")\n",
    "print(\"Total Rows in Data: \" + str(count_rows))\n",
    "print(f\"Number of rows above 99th percentile: {count_above_99}\")\n",
    "print(f\"% above 99th percentile: {percent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many accounts have at least 1 row with a scope 3 emissions amount in the 99th percentile or higher\n",
    "# Calculate the overall 99th percentile\n",
    "overall_99th = outlier_df_cleaned['Scope_3_emissions_amount'].quantile(0.99)\n",
    "\n",
    "# Find accounts with at least one emission value above 99th percentile\n",
    "high_emission_accounts_99 = outlier_df_cleaned[\n",
    "    outlier_df_cleaned['Scope_3_emissions_amount'] >= overall_99th\n",
    "]['account_id'].unique()\n",
    "\n",
    "# Count these accounts\n",
    "n_high_accounts = len(high_emission_accounts_99)\n",
    "\n",
    "print(f\"Number of accounts with emissions in 99th percentile or higher: {n_high_accounts}\")\n",
    "print(f\"Percentage of total accounts: {(n_high_accounts/outlier_df_cleaned['account_id'].nunique()*100):.1f}%\")\n",
    "print(f\"\\n99th percentile threshold: {overall_99th:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extreme positive skew in the data is difficult to correct for without . The data must always be analyzed with consideration of Primary activity, as extreme differences in each category is expected. However, I think it is best to remove any account that has observations in the 99th percentile or higher since 52 million is not likely to be a valid amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all accounts that have at least 1 value in the 99th percentile\n",
    "# Find accounts with at least one emission value above 99th percentile\n",
    "accounts_to_remove = outlier_df_cleaned[\n",
    "    outlier_df_cleaned['Scope_3_emissions_amount'] >= overall_99th\n",
    "]['account_id'].unique()\n",
    "\n",
    "# Create new dataframe excluding these accounts\n",
    "filtered_df = outlier_df_cleaned[~outlier_df_cleaned['account_id'].isin(accounts_to_remove)]\n",
    "\n",
    "print(f\"Original number of accounts: {outlier_df_cleaned['account_id'].nunique()}\")\n",
    "print(f\"Number of accounts removed: {len(accounts_to_remove)}\")\n",
    "print(f\"Remaining number of accounts: {filtered_df['account_id'].nunique()}\")\n",
    "print(f\"\\n99th percentile threshold: {overall_99th:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to re-visualize the data with these accounts removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the percentiles\n",
    "percentiles = [25, 50, 80, 90, 95, 96, 97, 98, 99, 100]\n",
    "percentile_values = {p: filtered_df['Scope_3_emissions_amount'].quantile(p/100) for p in percentiles}\n",
    "\n",
    "# Create color scale for different percentile ranges\n",
    "color_scale = ['#d4e6f1','#a9cce3','#7fb3d5','#5499c7','#2980b9', '#2471a3', '#1f618d', '#1a5276', '#1b4f72', '#641e16']\n",
    "\n",
    "# Initialize the figure\n",
    "fig = go.Figure()\n",
    "#1b4f72\n",
    "# Create percentile ranges and add traces for each range\n",
    "prev_percentile = 0\n",
    "for i, percentile in enumerate(percentiles):\n",
    "    if i == 0:\n",
    "        mask = filtered_df['Scope_3_emissions_amount'] <= percentile_values[percentile]\n",
    "        start = 0\n",
    "    else:\n",
    "        mask = (filtered_df['Scope_3_emissions_amount'] > percentile_values[prev_percentile]) & \\\n",
    "               (filtered_df['Scope_3_emissions_amount'] <= percentile_values[percentile])\n",
    "        start = prev_percentile\n",
    "    \n",
    "    data = filtered_df[mask].copy()\n",
    "    data['percentile_range'] = f\"{start}-{percentile}\"\n",
    "    \n",
    "    if len(data) > 0:  # Only add trace if there is data in this range\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=data.index,\n",
    "                y=data['Scope_3_emissions_amount'],\n",
    "                mode='markers',\n",
    "                name=f\"{start}-{percentile}th percentile\",\n",
    "                marker=dict(\n",
    "                    color=color_scale[i] if i < len(color_scale) else color_scale[-1],\n",
    "                    size=6\n",
    "                ),\n",
    "                hovertemplate=(\n",
    "                    '<b>Account ID:</b> %{customdata[0]}<br>' +\n",
    "                    '<b>Account Name:</b> %{customdata[1]}<br>' +\n",
    "                    '<b>Primary Activity:</b> %{customdata[2]}<br>' +\n",
    "                    '<b>Scope 3 Emissions:</b> %{y:,.2f}<br>' +\n",
    "                    '<b>Percentile Range:</b> %{customdata[3]}<br>'\n",
    "                ),\n",
    "                customdata=np.column_stack((\n",
    "                    data['account_id'],\n",
    "                    data['account_name'],\n",
    "                    data['Primary activity'],\n",
    "                    data['percentile_range']\n",
    "                ))\n",
    "            )\n",
    "        )\n",
    "    prev_percentile = percentile\n",
    "\n",
    "# Add lines for each percentile\n",
    "for percentile, value in percentile_values.items():\n",
    "    fig.add_hline(\n",
    "        y=value,\n",
    "        line_dash=\"dash\",\n",
    "        line_color=\"gray\",\n",
    "        line_width=1,\n",
    "        annotation=dict(\n",
    "            text=f\"{percentile}th percentile\",\n",
    "            xref=\"paper\",\n",
    "            yref=\"y\",\n",
    "            x=1.02,\n",
    "            y=value,\n",
    "            showarrow=False,\n",
    "            font=dict(size=8)\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    width=1200,\n",
    "    title_text=\"Scope 3 Emissions Distribution by Percentile Ranges\",\n",
    "    xaxis_title=\"Index\",\n",
    "    yaxis_title=\"Scope 3 Emissions Amount\",\n",
    "    showlegend=True,\n",
    "    legend_title=\"Percentile Ranges\",\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=1.02,\n",
    "        bgcolor=\"rgba(255, 255, 255, 0.8)\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "for p in percentiles:\n",
    "    print(f\"{p}th percentile: {percentile_values[p]:,.2f}\")\n",
    "print(f\"\\nTotal number of observations: {len(filtered_df)}\")\n",
    "print(f\"Number of unique accounts: {filtered_df['account_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the histogram again\n",
    "sns.histplot(data=filtered_df, x='Scope_3_emissions_amount', bins=40, kde=True)\n",
    "# Customize the plot\n",
    "plt.title('Distribution of Amount')\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Frequency')\n",
    "# Add grid for better readability\n",
    "plt.grid(True, alpha=0.3)\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# final df stats\n",
    "print(filtered_df['Scope_3_emissions_amount'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Analyze the Change*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the change in standard deviation after outlier removal\n",
    "std_now = filtered_df['Scope_3_emissions_amount'].std()\n",
    "std_before = data_clean['Scope_3_emissions_amount'].std()\n",
    "std_difference = std_now - std_before\n",
    "\n",
    "print(f\"New Standard Deviation: {std_now:,.2f}\")\n",
    "print(f\"Original Standard Deviation: {std_before:,.2f}\")\n",
    "print(f\"Difference in Standard Deviations: {std_difference:,.2f}\")\n",
    "print(\"--------------------------\")\n",
    "# Calculate skewness and kurtosis\n",
    "skew_now = filtered_df['Scope_3_emissions_amount'].skew()\n",
    "kurtosis_now = filtered_df['Scope_3_emissions_amount'].kurtosis()\n",
    "\n",
    "# Calculate change\n",
    "skew_diff = skew_now-original_skew\n",
    "kurt_diff = kurtosis_now-original_kurtosis\n",
    "\n",
    "print(f\"New Skew: {skew_now:,.2f}\")\n",
    "print(f\"Original Skew: {original_skew:,.2f}\")\n",
    "print(f\"Difference in Skew: {skew_diff:,.2f}\")\n",
    "print(\"--------------------------\")\n",
    "print(f\"New Kurtosis: {kurtosis_now:,.2f}\")\n",
    "print(f\"Original Kurtosis: {original_kurtosis:,.2f}\")\n",
    "print(f\"Difference in Kurtosis: {kurt_diff:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the distribution is still far from normal and extreme positive skewness remains, the standard deviation has been reduced by 328 million from the original dataset and the new skew is 8.03 and the new kurtosis is 77.54 (very high).\n",
    "\n",
    "This is a significant change to the skew and distribution, but without knowing enough about the underlying data it will take a lot of time and research to fully understand the best way to handle skewness and outliers, whether that is through imputation, more vast removal, industry SME validation, or manual data corrections via data research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Final Step*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final data, rather than having a GDP, GHG, and Population for each year, I am going to create an average so each country has a consistent value for these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename back to df\n",
    "df = filtered_df\n",
    "\n",
    "# Create an average of all years by country for gdp, ghg, and population\n",
    "df['country_ghg_avg'] = df.groupby('incorporated_country')['country_total_ghg'].transform('mean')\n",
    "df['country_population_avg'] = df.groupby('incorporated_country')['country_population'].transform('mean')\n",
    "df['country_gdp_avg'] = df.groupby('incorporated_country')['country_gdp'].transform('mean')\n",
    "\n",
    "# Drop original columns\n",
    "df = df.drop(['country_total_ghg', 'country_population', 'country_gdp'], axis=1)\n",
    "\n",
    "df.columns.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "df.to_csv('GHG_Post_Outlier.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compressed csv\n",
    "df.to_csv('GHG_Post_Outlier.csv.gz', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
